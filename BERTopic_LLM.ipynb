{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f624533e",
   "metadata": {},
   "source": [
    "Backup from Google Colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "359ee94b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bertopic in c:\\users\\user\\anaconda3\\lib\\site-packages (0.16.4)\n",
      "Requirement already satisfied: gensim in c:\\users\\user\\anaconda3\\lib\\site-packages (4.3.3)\n",
      "Collecting openai\n",
      "  Downloading openai-1.68.2-py3-none-any.whl (606 kB)\n",
      "Requirement already satisfied: llama-cpp-python in c:\\users\\user\\anaconda3\\lib\\site-packages (0.3.8)\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from bertopic) (0.5.7)\n",
      "Requirement already satisfied: plotly>=4.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from bertopic) (6.0.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from bertopic) (1.24.4)\n",
      "Requirement already satisfied: sentence-transformers>=0.4.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from bertopic) (3.2.1)\n",
      "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from bertopic) (2.0.3)\n",
      "Requirement already satisfied: hdbscan>=0.8.29 in c:\\users\\user\\anaconda3\\lib\\site-packages (from bertopic) (0.8.40)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from bertopic) (1.3.2)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from bertopic) (4.50.2)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Collecting pydantic<3,>=1.9.0\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Collecting sniffio\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\user\\anaconda3\\lib\\site-packages (from openai) (4.12.2)\n",
      "Collecting jiter<1,>=0.4.0\n",
      "  Downloading jiter-0.9.0-cp38-cp38-win_amd64.whl (198 kB)\n",
      "Collecting anyio<5,>=3.5.0\n",
      "  Downloading anyio-4.5.2-py3-none-any.whl (89 kB)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: numba>=0.51.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.58.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from plotly>=4.7.0->bertopic) (1.30.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from plotly>=4.7.0->bertopic) (20.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (2.4.1+cu118)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (4.46.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (9.5.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.29.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2020.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2025.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2.post1->bertopic) (2.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\user\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.11.2)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.27.2\n",
      "  Downloading pydantic_core-2.27.2-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Collecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2020.6.20)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2.10)\n",
      "Collecting exceptiongroup>=1.0.2; python_version < \"3.11\"\n",
      "  Using cached exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.41.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.9\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (8.5.0)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from packaging->plotly>=4.7.0->bertopic) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from packaging->plotly>=4.7.0->bertopic) (2.4.7)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.0.12)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2025.3.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.5)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.6.2)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.20.3)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2.24.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2020.10.15)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (5.3.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.5.3)\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\user\\anaconda3\\lib\\site-packages (from importlib-metadata; python_version < \"3.9\"->numba>=0.51.2->umap-learn>=0.5.0->bertopic) (3.20.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from networkx->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (4.4.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.1.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (3.0.4)\n",
      "Installing collected packages: annotated-types, pydantic-core, pydantic, distro, h11, httpcore, exceptiongroup, sniffio, anyio, httpx, jiter, openai\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.5.2 distro-1.9.0 exceptiongroup-1.2.2 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 jiter-0.9.0 openai-1.68.2 pydantic-2.10.6 pydantic-core-2.27.2 sniffio-1.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install bertopic gensim openai llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d48552a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "import umap\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "168905ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"C:/Users/User/OneDrive - Singapore Management University/Semester 5/ISSS609 Text Analytics and Applications/Group Project/Amazon Fine Food Reviews/topic_model_sample_391k.csv\"\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb7f9554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure correct column name\n",
    "if \"Text\" not in df.columns:\n",
    "    raise ValueError(\"Column 'Text' not found in dataset\")\n",
    "\n",
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Text preprocessing function (Stopword removal is disabled to retain context)\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', str(text))  # Remove extra spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())  # Remove special characters, numbers\n",
    "    words = simple_preprocess(text)  # Tokenize\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if len(word) > 2]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply text preprocessing\n",
    "df[\"processed_text\"] = df[\"Text\"].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65607e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from C:/Users/User/OneDrive - Singapore Management University/Semester 5/ISSS609 Text Analytics and Applications/Group Project/Amazon Fine Food Reviews/LLM/zephyr-7b-alpha.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = huggingfaceh4_zephyr-7b-alpha\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = huggingfaceh4_zephyr-7b-alpha\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 2 '</s>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      "...............................................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 8192\n",
      "llama_init_from_model: n_ctx_per_seq = 8192\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_init_from_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   560.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'huggingfaceh4_zephyr-7b-alpha', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '2'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from bertopic.representation import LlamaCPP\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Custom prompt for LlamaCPP\n",
    "summ_prompt = \"\"\"\n",
    "Q: I have a topic that contains the following documents:\n",
    "[DOCUMENTS]\n",
    "\n",
    "The topic is described by the following keywords: '[KEYWORDS]'.\n",
    "\n",
    "Based on the above information, can you give a short label of the topic?\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "# Load the LLaMA model locally\n",
    "llm = Llama(model_path=\"C:/Users/User/OneDrive - Singapore Management University/Semester 5/ISSS609 Text Analytics and Applications/Group Project/Amazon Fine Food Reviews/LLM/zephyr-7b-alpha.Q4_K_M.gguf\", n_gpu_layers=-1, n_ctx=8192, stop=\"Q:\")\n",
    "\n",
    "# Create the representation model using LlamaCPP\n",
    "representation_model = LlamaCPP(llm, prompt=summ_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3dae0828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 23:42:17,109 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ae4366345942afb9c894c3dfd5200a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Batches'), FloatProgress(value=0.0, max=12221.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 23:45:24,311 - BERTopic - Embedding - Completed ✓\n",
      "2025-03-22 23:45:24,311 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-03-22 23:54:11,552 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-03-22 23:54:11,576 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-03-23 00:04:10,733 - BERTopic - Cluster - Completed ✓\n",
      "2025-03-23 00:04:10,867 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "  0%|                                                                                          | 0/152 [00:00<?, ?it/s]llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   75108.63 ms /  1630 tokens (   46.08 ms per token,    21.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2478.85 ms /    15 runs   (  165.26 ms per token,     6.05 tokens per second)\n",
      "llama_perf_context_print:       total time =   78218.58 ms /  1645 tokens\n",
      "  1%|▌                                                                               | 1/152 [01:18<3:17:38, 78.54s/it]Llama.generate: 17 prefix-match hit, remaining 1820 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   83489.31 ms /  1820 tokens (   45.87 ms per token,    21.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2382.71 ms /    15 runs   (  158.85 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =   85892.18 ms /  1835 tokens\n",
      "  1%|█                                                                               | 2/152 [02:44<3:21:59, 80.79s/it]Llama.generate: 17 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   20284.31 ms /   469 tokens (   43.25 ms per token,    23.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2215.49 ms /    15 runs   (  147.70 ms per token,     6.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   22563.77 ms /   484 tokens\n",
      "  2%|█▌                                                                              | 3/152 [03:07<2:37:17, 63.34s/it]Llama.generate: 17 prefix-match hit, remaining 704 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   30866.90 ms /   704 tokens (   43.85 ms per token,    22.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2203.12 ms /    15 runs   (  146.87 ms per token,     6.81 tokens per second)\n",
      "llama_perf_context_print:       total time =   33102.40 ms /   719 tokens\n",
      "  3%|██                                                                              | 4/152 [03:40<2:13:52, 54.27s/it]Llama.generate: 17 prefix-match hit, remaining 1062 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   47221.02 ms /  1062 tokens (   44.46 ms per token,    22.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2296.45 ms /    15 runs   (  153.10 ms per token,     6.53 tokens per second)\n",
      "llama_perf_context_print:       total time =   49551.81 ms /  1077 tokens\n",
      "  3%|██▋                                                                             | 5/152 [04:30<2:09:42, 52.94s/it]Llama.generate: 17 prefix-match hit, remaining 870 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   38778.15 ms /   870 tokens (   44.57 ms per token,    22.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1900.04 ms /    13 runs   (  146.16 ms per token,     6.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   40687.16 ms /   883 tokens\n",
      "  4%|███▏                                                                            | 6/152 [05:10<1:59:53, 49.27s/it]Llama.generate: 17 prefix-match hit, remaining 1711 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   77687.54 ms /  1711 tokens (   45.40 ms per token,    22.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2308.43 ms /    15 runs   (  153.90 ms per token,     6.50 tokens per second)\n",
      "llama_perf_context_print:       total time =   80040.37 ms /  1726 tokens\n",
      "  5%|███▋                                                                            | 7/152 [06:31<2:21:27, 58.53s/it]Llama.generate: 17 prefix-match hit, remaining 1387 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   62358.76 ms /  1387 tokens (   44.96 ms per token,    22.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2259.70 ms /    15 runs   (  150.65 ms per token,     6.64 tokens per second)\n",
      "llama_perf_context_print:       total time =   64651.29 ms /  1402 tokens\n",
      "  5%|████▏                                                                           | 8/152 [07:35<2:24:57, 60.40s/it]Llama.generate: 17 prefix-match hit, remaining 507 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   21934.08 ms /   507 tokens (   43.26 ms per token,    23.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2317.49 ms /    15 runs   (  154.50 ms per token,     6.47 tokens per second)\n",
      "llama_perf_context_print:       total time =   24261.86 ms /   522 tokens\n",
      "  6%|████▋                                                                           | 9/152 [08:00<1:58:10, 49.59s/it]Llama.generate: 17 prefix-match hit, remaining 1553 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   70069.60 ms /  1553 tokens (   45.12 ms per token,    22.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1979.32 ms /    13 runs   (  152.26 ms per token,     6.57 tokens per second)\n",
      "llama_perf_context_print:       total time =   72061.06 ms /  1566 tokens\n",
      "  7%|█████▏                                                                         | 10/152 [09:12<2:13:24, 56.37s/it]Llama.generate: 17 prefix-match hit, remaining 1456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   65789.03 ms /  1456 tokens (   45.18 ms per token,    22.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2289.25 ms /    15 runs   (  152.62 ms per token,     6.55 tokens per second)\n",
      "llama_perf_context_print:       total time =   68100.67 ms /  1471 tokens\n",
      "  7%|█████▋                                                                         | 11/152 [10:20<2:20:45, 59.90s/it]Llama.generate: 17 prefix-match hit, remaining 1160 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   52064.51 ms /  1160 tokens (   44.88 ms per token,    22.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2269.66 ms /    15 runs   (  151.31 ms per token,     6.61 tokens per second)\n",
      "llama_perf_context_print:       total time =   54353.67 ms /  1175 tokens\n",
      "  8%|██████▏                                                                        | 12/152 [11:14<2:15:57, 58.27s/it]Llama.generate: 17 prefix-match hit, remaining 2270 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =  105501.98 ms /  2270 tokens (   46.48 ms per token,    21.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2406.79 ms /    15 runs   (  160.45 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =  107930.25 ms /  2285 tokens\n",
      "  9%|██████▊                                                                        | 13/152 [13:02<2:49:32, 73.18s/it]Llama.generate: 17 prefix-match hit, remaining 1497 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   67843.77 ms /  1497 tokens (   45.32 ms per token,    22.07 tokens per second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        eval time =    2326.92 ms /    15 runs   (  155.13 ms per token,     6.45 tokens per second)\n",
      "llama_perf_context_print:       total time =   70191.82 ms /  1512 tokens\n",
      "  9%|███████▎                                                                       | 14/152 [14:13<2:46:16, 72.29s/it]Llama.generate: 17 prefix-match hit, remaining 1401 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   63103.65 ms /  1401 tokens (   45.04 ms per token,    22.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2297.95 ms /    15 runs   (  153.20 ms per token,     6.53 tokens per second)\n",
      "llama_perf_context_print:       total time =   65411.29 ms /  1416 tokens\n",
      " 10%|███████▊                                                                       | 15/152 [15:18<2:40:22, 70.24s/it]Llama.generate: 17 prefix-match hit, remaining 1096 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   49145.61 ms /  1096 tokens (   44.84 ms per token,    22.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2220.32 ms /    15 runs   (  148.02 ms per token,     6.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   51392.59 ms /  1111 tokens\n",
      " 11%|████████▎                                                                      | 16/152 [16:10<2:26:29, 64.63s/it]Llama.generate: 17 prefix-match hit, remaining 1562 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   71060.07 ms /  1562 tokens (   45.49 ms per token,    21.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2299.64 ms /    15 runs   (  153.31 ms per token,     6.52 tokens per second)\n",
      "llama_perf_context_print:       total time =   73374.60 ms /  1577 tokens\n",
      " 11%|████████▊                                                                      | 17/152 [17:23<2:31:20, 67.26s/it]Llama.generate: 17 prefix-match hit, remaining 1353 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   60213.61 ms /  1353 tokens (   44.50 ms per token,    22.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2298.20 ms /    15 runs   (  153.21 ms per token,     6.53 tokens per second)\n",
      "llama_perf_context_print:       total time =   62521.71 ms /  1368 tokens\n",
      " 12%|█████████▎                                                                     | 18/152 [18:26<2:27:03, 65.85s/it]Llama.generate: 17 prefix-match hit, remaining 931 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   41196.30 ms /   931 tokens (   44.25 ms per token,    22.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2195.26 ms /    15 runs   (  146.35 ms per token,     6.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   43405.01 ms /   946 tokens\n",
      " 12%|█████████▉                                                                     | 19/152 [19:09<2:11:03, 59.12s/it]Llama.generate: 17 prefix-match hit, remaining 1445 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   65363.65 ms /  1445 tokens (   45.23 ms per token,    22.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2309.65 ms /    15 runs   (  153.98 ms per token,     6.49 tokens per second)\n",
      "llama_perf_context_print:       total time =   67683.23 ms /  1460 tokens\n",
      " 13%|██████████▍                                                                    | 20/152 [20:17<2:15:43, 61.69s/it]Llama.generate: 17 prefix-match hit, remaining 1049 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   46700.82 ms /  1049 tokens (   44.52 ms per token,    22.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2210.59 ms /    15 runs   (  147.37 ms per token,     6.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   48920.64 ms /  1064 tokens\n",
      " 14%|██████████▉                                                                    | 21/152 [21:06<2:06:21, 57.87s/it]Llama.generate: 17 prefix-match hit, remaining 1132 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   50200.97 ms /  1132 tokens (   44.35 ms per token,    22.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2263.41 ms /    15 runs   (  150.89 ms per token,     6.63 tokens per second)\n",
      "llama_perf_context_print:       total time =   52482.95 ms /  1147 tokens\n",
      " 14%|███████████▍                                                                   | 22/152 [21:58<2:01:54, 56.26s/it]Llama.generate: 17 prefix-match hit, remaining 1152 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   51229.79 ms /  1152 tokens (   44.47 ms per token,    22.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2320.24 ms /    15 runs   (  154.68 ms per token,     6.46 tokens per second)\n",
      "llama_perf_context_print:       total time =   53569.52 ms /  1167 tokens\n",
      " 15%|███████████▉                                                                   | 23/152 [22:52<1:59:14, 55.46s/it]Llama.generate: 17 prefix-match hit, remaining 1043 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   46921.62 ms /  1043 tokens (   44.99 ms per token,    22.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2250.81 ms /    15 runs   (  150.05 ms per token,     6.66 tokens per second)\n",
      "llama_perf_context_print:       total time =   49181.81 ms /  1058 tokens\n",
      " 16%|████████████▍                                                                  | 24/152 [23:41<1:54:19, 53.59s/it]Llama.generate: 17 prefix-match hit, remaining 1969 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   90528.34 ms /  1969 tokens (   45.98 ms per token,    21.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2341.09 ms /    15 runs   (  156.07 ms per token,     6.41 tokens per second)\n",
      "llama_perf_context_print:       total time =   92878.74 ms /  1984 tokens\n",
      " 16%|████████████▉                                                                  | 25/152 [25:14<2:18:23, 65.38s/it]Llama.generate: 17 prefix-match hit, remaining 1141 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   51011.04 ms /  1141 tokens (   44.71 ms per token,    22.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2333.25 ms /    15 runs   (  155.55 ms per token,     6.43 tokens per second)\n",
      "llama_perf_context_print:       total time =   53363.88 ms /  1156 tokens\n",
      " 17%|█████████████▌                                                                 | 26/152 [26:07<2:09:45, 61.79s/it]Llama.generate: 17 prefix-match hit, remaining 672 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   29098.84 ms /   672 tokens (   43.30 ms per token,    23.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2360.25 ms /    15 runs   (  157.35 ms per token,     6.36 tokens per second)\n",
      "llama_perf_context_print:       total time =   31468.65 ms /   687 tokens\n",
      " 18%|██████████████                                                                 | 27/152 [26:39<1:49:46, 52.70s/it]Llama.generate: 17 prefix-match hit, remaining 1412 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   63217.22 ms /  1412 tokens (   44.77 ms per token,    22.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2331.23 ms /    15 runs   (  155.42 ms per token,     6.43 tokens per second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:       total time =   65574.06 ms /  1427 tokens\n",
      " 18%|██████████████▌                                                                | 28/152 [27:44<1:56:53, 56.56s/it]Llama.generate: 17 prefix-match hit, remaining 2018 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   93333.93 ms /  2018 tokens (   46.25 ms per token,    21.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2349.15 ms /    15 runs   (  156.61 ms per token,     6.39 tokens per second)\n",
      "llama_perf_context_print:       total time =   95699.36 ms /  2033 tokens\n",
      " 19%|███████████████                                                                | 29/152 [29:20<2:20:01, 68.31s/it]Llama.generate: 17 prefix-match hit, remaining 838 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   37070.88 ms /   838 tokens (   44.24 ms per token,    22.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2189.17 ms /    15 runs   (  145.94 ms per token,     6.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   39277.96 ms /   853 tokens\n",
      " 20%|███████████████▌                                                               | 30/152 [29:59<2:01:13, 59.62s/it]Llama.generate: 17 prefix-match hit, remaining 748 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   32755.89 ms /   748 tokens (   43.79 ms per token,    22.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2181.27 ms /    15 runs   (  145.42 ms per token,     6.88 tokens per second)\n",
      "llama_perf_context_print:       total time =   34957.45 ms /   763 tokens\n",
      " 20%|████████████████                                                               | 31/152 [30:34<1:45:19, 52.23s/it]Llama.generate: 17 prefix-match hit, remaining 1596 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   72081.97 ms /  1596 tokens (   45.16 ms per token,    22.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2283.02 ms /    15 runs   (  152.20 ms per token,     6.57 tokens per second)\n",
      "llama_perf_context_print:       total time =   74374.63 ms /  1611 tokens\n",
      " 21%|████████████████▋                                                              | 32/152 [31:49<1:57:45, 58.88s/it]Llama.generate: 17 prefix-match hit, remaining 1695 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   77119.49 ms /  1695 tokens (   45.50 ms per token,    21.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1841.27 ms /    12 runs   (  153.44 ms per token,     6.52 tokens per second)\n",
      "llama_perf_context_print:       total time =   78968.52 ms /  1707 tokens\n",
      " 22%|█████████████████▏                                                             | 33/152 [33:08<2:08:44, 64.91s/it]Llama.generate: 17 prefix-match hit, remaining 1261 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   56481.84 ms /  1261 tokens (   44.79 ms per token,    22.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2380.70 ms /    15 runs   (  158.71 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =   58872.11 ms /  1276 tokens\n",
      " 22%|█████████████████▋                                                             | 34/152 [34:07<2:04:06, 63.11s/it]Llama.generate: 17 prefix-match hit, remaining 2326 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =  108603.30 ms /  2326 tokens (   46.69 ms per token,    21.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2436.35 ms /    15 runs   (  162.42 ms per token,     6.16 tokens per second)\n",
      "llama_perf_context_print:       total time =  111053.56 ms /  2341 tokens\n",
      " 23%|██████████████████▏                                                            | 35/152 [35:58<2:31:07, 77.50s/it]Llama.generate: 17 prefix-match hit, remaining 1521 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   69183.90 ms /  1521 tokens (   45.49 ms per token,    21.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2280.75 ms /    15 runs   (  152.05 ms per token,     6.58 tokens per second)\n",
      "llama_perf_context_print:       total time =   71474.22 ms /  1536 tokens\n",
      " 24%|██████████████████▋                                                            | 36/152 [37:09<2:26:20, 75.69s/it]Llama.generate: 17 prefix-match hit, remaining 1133 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   50103.33 ms /  1133 tokens (   44.22 ms per token,    22.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2420.33 ms /    15 runs   (  161.36 ms per token,     6.20 tokens per second)\n",
      "llama_perf_context_print:       total time =   52533.46 ms /  1148 tokens\n",
      " 24%|███████████████████▏                                                           | 37/152 [38:02<2:11:45, 68.75s/it]Llama.generate: 17 prefix-match hit, remaining 1433 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   64270.74 ms /  1433 tokens (   44.85 ms per token,    22.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2276.73 ms /    15 runs   (  151.78 ms per token,     6.59 tokens per second)\n",
      "llama_perf_context_print:       total time =   66556.70 ms /  1448 tokens\n",
      " 25%|███████████████████▊                                                           | 38/152 [39:08<2:09:22, 68.09s/it]Llama.generate: 17 prefix-match hit, remaining 2115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   99933.35 ms /  2115 tokens (   47.25 ms per token,    21.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2404.16 ms /    15 runs   (  160.28 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =  102350.40 ms /  2130 tokens\n",
      " 26%|████████████████████▎                                                          | 39/152 [40:51<2:27:36, 78.37s/it]Llama.generate: 17 prefix-match hit, remaining 1072 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   48038.48 ms /  1072 tokens (   44.81 ms per token,    22.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2216.56 ms /    15 runs   (  147.77 ms per token,     6.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   50270.19 ms /  1087 tokens\n",
      " 26%|████████████████████▊                                                          | 40/152 [41:41<2:10:34, 69.95s/it]Llama.generate: 17 prefix-match hit, remaining 1416 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   63557.17 ms /  1416 tokens (   44.89 ms per token,    22.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2260.79 ms /    15 runs   (  150.72 ms per token,     6.63 tokens per second)\n",
      "llama_perf_context_print:       total time =   65835.39 ms /  1431 tokens\n",
      " 27%|█████████████████████▎                                                         | 41/152 [42:47<2:07:07, 68.72s/it]Llama.generate: 17 prefix-match hit, remaining 1612 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   73752.23 ms /  1612 tokens (   45.75 ms per token,    21.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2303.53 ms /    15 runs   (  153.57 ms per token,     6.51 tokens per second)\n",
      "llama_perf_context_print:       total time =   76073.51 ms /  1627 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|█████████████████████▊                                                         | 42/152 [44:03<2:10:03, 70.94s/it]Llama.generate: 18 prefix-match hit, remaining 1352 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   63352.37 ms /  1352 tokens (   46.86 ms per token,    21.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2619.31 ms /    15 runs   (  174.62 ms per token,     5.73 tokens per second)\n",
      "llama_perf_context_print:       total time =   65981.80 ms /  1367 tokens\n",
      " 28%|██████████████████████▎                                                        | 43/152 [45:09<2:06:10, 69.46s/it]Llama.generate: 17 prefix-match hit, remaining 893 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   40316.75 ms /   893 tokens (   45.15 ms per token,    22.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2259.28 ms /    15 runs   (  150.62 ms per token,     6.64 tokens per second)\n",
      "llama_perf_context_print:       total time =   42598.17 ms /   908 tokens\n",
      " 29%|██████████████████████▊                                                        | 44/152 [45:52<1:50:36, 61.45s/it]Llama.generate: 17 prefix-match hit, remaining 1902 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   87548.82 ms /  1902 tokens (   46.03 ms per token,    21.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2356.36 ms /    15 runs   (  157.09 ms per token,     6.37 tokens per second)\n",
      "llama_perf_context_print:       total time =   89973.90 ms /  1917 tokens\n",
      " 30%|███████████████████████▍                                                       | 45/152 [47:22<2:04:51, 70.01s/it]Llama.generate: 17 prefix-match hit, remaining 1519 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   68359.79 ms /  1519 tokens (   45.00 ms per token,    22.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2401.23 ms /    15 runs   (  160.08 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =   70773.00 ms /  1534 tokens\n",
      " 30%|███████████████████████▉                                                       | 46/152 [48:32<2:04:05, 70.24s/it]Llama.generate: 17 prefix-match hit, remaining 1194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   54335.70 ms /  1194 tokens (   45.51 ms per token,    21.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2651.73 ms /    15 runs   (  176.78 ms per token,     5.66 tokens per second)\n",
      "llama_perf_context_print:       total time =   56997.94 ms /  1209 tokens\n",
      " 31%|████████████████████████▍                                                      | 47/152 [49:29<1:55:58, 66.28s/it]Llama.generate: 17 prefix-match hit, remaining 1441 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   66573.07 ms /  1441 tokens (   46.20 ms per token,    21.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2395.75 ms /    15 runs   (  159.72 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =   68979.13 ms /  1456 tokens\n",
      " 32%|████████████████████████▉                                                      | 48/152 [50:38<1:56:17, 67.09s/it]Llama.generate: 17 prefix-match hit, remaining 1757 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   80766.89 ms /  1757 tokens (   45.97 ms per token,    21.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2668.73 ms /    15 runs   (  177.92 ms per token,     5.62 tokens per second)\n",
      "llama_perf_context_print:       total time =   83458.05 ms /  1772 tokens\n",
      " 32%|█████████████████████████▍                                                     | 49/152 [52:02<2:03:36, 72.01s/it]Llama.generate: 17 prefix-match hit, remaining 1066 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   47627.26 ms /  1066 tokens (   44.68 ms per token,    22.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2323.86 ms /    15 runs   (  154.92 ms per token,     6.45 tokens per second)\n",
      "llama_perf_context_print:       total time =   49971.42 ms /  1081 tokens\n",
      " 33%|█████████████████████████▉                                                     | 50/152 [52:52<1:51:10, 65.40s/it]Llama.generate: 17 prefix-match hit, remaining 1619 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   73582.04 ms /  1619 tokens (   45.45 ms per token,    22.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2396.07 ms /    15 runs   (  159.74 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =   76010.06 ms /  1634 tokens\n",
      " 34%|██████████████████████████▌                                                    | 51/152 [54:08<1:55:28, 68.60s/it]Llama.generate: 17 prefix-match hit, remaining 902 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   39947.55 ms /   902 tokens (   44.29 ms per token,    22.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2194.51 ms /    15 runs   (  146.30 ms per token,     6.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   42151.47 ms /   917 tokens\n",
      " 34%|███████████████████████████                                                    | 52/152 [54:50<1:41:06, 60.67s/it]Llama.generate: 17 prefix-match hit, remaining 1760 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   80562.83 ms /  1760 tokens (   45.77 ms per token,    21.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2304.58 ms /    15 runs   (  153.64 ms per token,     6.51 tokens per second)\n",
      "llama_perf_context_print:       total time =   82885.81 ms /  1775 tokens\n",
      " 35%|███████████████████████████▌                                                   | 53/152 [56:13<1:51:06, 67.34s/it]Llama.generate: 17 prefix-match hit, remaining 1144 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   51165.04 ms /  1144 tokens (   44.72 ms per token,    22.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2228.56 ms /    15 runs   (  148.57 ms per token,     6.73 tokens per second)\n",
      "llama_perf_context_print:       total time =   53414.69 ms /  1159 tokens\n",
      " 36%|████████████████████████████                                                   | 54/152 [57:06<1:43:10, 63.17s/it]Llama.generate: 17 prefix-match hit, remaining 1293 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   57583.19 ms /  1293 tokens (   44.53 ms per token,    22.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2255.12 ms /    15 runs   (  150.34 ms per token,     6.65 tokens per second)\n",
      "llama_perf_context_print:       total time =   59848.41 ms /  1308 tokens\n",
      " 36%|████████████████████████████▌                                                  | 55/152 [58:06<1:40:30, 62.17s/it]Llama.generate: 17 prefix-match hit, remaining 1777 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   81302.90 ms /  1777 tokens (   45.75 ms per token,    21.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2499.89 ms /    15 runs   (  166.66 ms per token,     6.00 tokens per second)\n",
      "llama_perf_context_print:       total time =   83812.63 ms /  1792 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|█████████████████████████████                                                  | 56/152 [59:30<1:49:52, 68.67s/it]Llama.generate: 17 prefix-match hit, remaining 1164 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   51717.80 ms /  1164 tokens (   44.43 ms per token,    22.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2314.62 ms /    15 runs   (  154.31 ms per token,     6.48 tokens per second)\n",
      "llama_perf_context_print:       total time =   54042.54 ms /  1179 tokens\n",
      " 38%|████████████████████████████▉                                                | 57/152 [1:00:24<1:41:47, 64.29s/it]Llama.generate: 17 prefix-match hit, remaining 2046 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   94230.40 ms /  2046 tokens (   46.06 ms per token,    21.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2433.97 ms /    15 runs   (  162.26 ms per token,     6.16 tokens per second)\n",
      "llama_perf_context_print:       total time =   96674.16 ms /  2061 tokens\n",
      " 38%|█████████████████████████████▍                                               | 58/152 [1:02:01<1:55:57, 74.02s/it]Llama.generate: 17 prefix-match hit, remaining 1492 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   66834.39 ms /  1492 tokens (   44.80 ms per token,    22.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2277.94 ms /    15 runs   (  151.86 ms per token,     6.58 tokens per second)\n",
      "llama_perf_context_print:       total time =   69121.93 ms /  1507 tokens\n",
      " 39%|█████████████████████████████▉                                               | 59/152 [1:03:10<1:52:29, 72.58s/it]Llama.generate: 17 prefix-match hit, remaining 1634 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   74351.73 ms /  1634 tokens (   45.50 ms per token,    21.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2356.85 ms /    15 runs   (  157.12 ms per token,     6.36 tokens per second)\n",
      "llama_perf_context_print:       total time =   76718.82 ms /  1649 tokens\n",
      " 39%|██████████████████████████████▍                                              | 60/152 [1:04:27<1:53:11, 73.82s/it]Llama.generate: 17 prefix-match hit, remaining 1294 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   58558.96 ms /  1294 tokens (   45.25 ms per token,    22.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2460.07 ms /    15 runs   (  164.00 ms per token,     6.10 tokens per second)\n",
      "llama_perf_context_print:       total time =   61028.54 ms /  1309 tokens\n",
      " 40%|██████████████████████████████▉                                              | 61/152 [1:05:28<1:46:09, 69.99s/it]Llama.generate: 17 prefix-match hit, remaining 1136 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   50296.45 ms /  1136 tokens (   44.28 ms per token,    22.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2256.73 ms /    15 runs   (  150.45 ms per token,     6.65 tokens per second)\n",
      "llama_perf_context_print:       total time =   52579.36 ms /  1151 tokens\n",
      " 41%|███████████████████████████████▍                                             | 62/152 [1:06:21<1:37:09, 64.77s/it]Llama.generate: 17 prefix-match hit, remaining 1237 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   55524.55 ms /  1237 tokens (   44.89 ms per token,    22.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2241.45 ms /    15 runs   (  149.43 ms per token,     6.69 tokens per second)\n",
      "llama_perf_context_print:       total time =   57786.17 ms /  1252 tokens\n",
      " 41%|███████████████████████████████▉                                             | 63/152 [1:07:18<1:32:58, 62.68s/it]Llama.generate: 17 prefix-match hit, remaining 1058 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   46949.00 ms /  1058 tokens (   44.38 ms per token,    22.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2224.80 ms /    15 runs   (  148.32 ms per token,     6.74 tokens per second)\n",
      "llama_perf_context_print:       total time =   49183.33 ms /  1073 tokens\n",
      " 42%|████████████████████████████████▍                                            | 64/152 [1:08:08<1:26:02, 58.66s/it]Llama.generate: 17 prefix-match hit, remaining 552 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   23763.02 ms /   552 tokens (   43.05 ms per token,    23.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2138.81 ms /    15 runs   (  142.59 ms per token,     7.01 tokens per second)\n",
      "llama_perf_context_print:       total time =   25921.13 ms /   567 tokens\n",
      " 43%|████████████████████████████████▉                                            | 65/152 [1:08:34<1:10:49, 48.84s/it]Llama.generate: 17 prefix-match hit, remaining 978 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   43448.71 ms /   978 tokens (   44.43 ms per token,    22.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2229.79 ms /    15 runs   (  148.65 ms per token,     6.73 tokens per second)\n",
      "llama_perf_context_print:       total time =   45687.95 ms /   993 tokens\n",
      " 43%|█████████████████████████████████▍                                           | 66/152 [1:09:19<1:08:39, 47.90s/it]Llama.generate: 17 prefix-match hit, remaining 1018 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   45315.46 ms /  1018 tokens (   44.51 ms per token,    22.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2244.62 ms /    15 runs   (  149.64 ms per token,     6.68 tokens per second)\n",
      "llama_perf_context_print:       total time =   47577.33 ms /  1033 tokens\n",
      " 44%|█████████████████████████████████▉                                           | 67/152 [1:10:07<1:07:43, 47.81s/it]Llama.generate: 17 prefix-match hit, remaining 872 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   38294.80 ms /   872 tokens (   43.92 ms per token,    22.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2214.21 ms /    15 runs   (  147.61 ms per token,     6.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   40518.64 ms /   887 tokens\n",
      " 45%|██████████████████████████████████▍                                          | 68/152 [1:10:47<1:03:52, 45.63s/it]Llama.generate: 17 prefix-match hit, remaining 1571 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   71527.69 ms /  1571 tokens (   45.53 ms per token,    21.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2607.77 ms /    15 runs   (  173.85 ms per token,     5.75 tokens per second)\n",
      "llama_perf_context_print:       total time =   74145.90 ms /  1586 tokens\n",
      " 45%|██████████████████████████████████▉                                          | 69/152 [1:12:02<1:14:57, 54.19s/it]Llama.generate: 17 prefix-match hit, remaining 1276 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   56934.60 ms /  1276 tokens (   44.62 ms per token,    22.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2248.85 ms /    15 runs   (  149.92 ms per token,     6.67 tokens per second)\n",
      "llama_perf_context_print:       total time =   59193.03 ms /  1291 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|███████████████████████████████████▍                                         | 70/152 [1:13:01<1:16:06, 55.69s/it]Llama.generate: 17 prefix-match hit, remaining 1619 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   73472.40 ms /  1619 tokens (   45.38 ms per token,    22.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2323.40 ms /    15 runs   (  154.89 ms per token,     6.46 tokens per second)\n",
      "llama_perf_context_print:       total time =   75805.15 ms /  1634 tokens\n",
      " 47%|███████████████████████████████████▉                                         | 71/152 [1:14:17<1:23:21, 61.75s/it]Llama.generate: 17 prefix-match hit, remaining 1057 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   47390.24 ms /  1057 tokens (   44.83 ms per token,    22.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2212.67 ms /    15 runs   (  147.51 ms per token,     6.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   49612.30 ms /  1072 tokens\n",
      " 47%|████████████████████████████████████▍                                        | 72/152 [1:15:06<1:17:28, 58.11s/it]Llama.generate: 17 prefix-match hit, remaining 1548 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   70144.98 ms /  1548 tokens (   45.31 ms per token,    22.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2274.88 ms /    15 runs   (  151.66 ms per token,     6.59 tokens per second)\n",
      "llama_perf_context_print:       total time =   72429.42 ms /  1563 tokens\n",
      " 48%|████████████████████████████████████▉                                        | 73/152 [1:16:19<1:22:10, 62.41s/it]Llama.generate: 17 prefix-match hit, remaining 845 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   37470.27 ms /   845 tokens (   44.34 ms per token,    22.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2242.67 ms /    15 runs   (  149.51 ms per token,     6.69 tokens per second)\n",
      "llama_perf_context_print:       total time =   39722.61 ms /   860 tokens\n",
      " 49%|█████████████████████████████████████▍                                       | 74/152 [1:16:58<1:12:17, 55.61s/it]Llama.generate: 17 prefix-match hit, remaining 1785 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   81084.59 ms /  1785 tokens (   45.43 ms per token,    22.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2351.28 ms /    15 runs   (  156.75 ms per token,     6.38 tokens per second)\n",
      "llama_perf_context_print:       total time =   83447.38 ms /  1800 tokens\n",
      " 49%|█████████████████████████████████████▉                                       | 75/152 [1:18:22<1:22:05, 63.96s/it]Llama.generate: 17 prefix-match hit, remaining 1211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   53984.30 ms /  1211 tokens (   44.58 ms per token,    22.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2089.70 ms /    14 runs   (  149.26 ms per token,     6.70 tokens per second)\n",
      "llama_perf_context_print:       total time =   56082.83 ms /  1225 tokens\n",
      " 50%|██████████████████████████████████████▌                                      | 76/152 [1:19:18<1:18:01, 61.60s/it]Llama.generate: 17 prefix-match hit, remaining 1567 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   71529.62 ms /  1567 tokens (   45.65 ms per token,    21.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2281.18 ms /    15 runs   (  152.08 ms per token,     6.58 tokens per second)\n",
      "llama_perf_context_print:       total time =   73820.37 ms /  1582 tokens\n",
      " 51%|███████████████████████████████████████                                      | 77/152 [1:20:32<1:21:35, 65.28s/it]Llama.generate: 17 prefix-match hit, remaining 1339 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   60183.84 ms /  1339 tokens (   44.95 ms per token,    22.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2409.95 ms /    15 runs   (  160.66 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =   62603.68 ms /  1354 tokens\n",
      " 51%|███████████████████████████████████████▌                                     | 78/152 [1:21:34<1:19:31, 64.48s/it]Llama.generate: 17 prefix-match hit, remaining 1521 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   68090.13 ms /  1521 tokens (   44.77 ms per token,    22.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2308.15 ms /    15 runs   (  153.88 ms per token,     6.50 tokens per second)\n",
      "llama_perf_context_print:       total time =   70407.68 ms /  1536 tokens\n",
      " 52%|████████████████████████████████████████                                     | 79/152 [1:22:45<1:20:37, 66.27s/it]Llama.generate: 17 prefix-match hit, remaining 1226 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   55055.46 ms /  1226 tokens (   44.91 ms per token,    22.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2237.07 ms /    15 runs   (  149.14 ms per token,     6.71 tokens per second)\n",
      "llama_perf_context_print:       total time =   57301.99 ms /  1241 tokens\n",
      " 53%|████████████████████████████████████████▌                                    | 80/152 [1:23:42<1:16:17, 63.58s/it]Llama.generate: 17 prefix-match hit, remaining 703 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   30976.29 ms /   703 tokens (   44.06 ms per token,    22.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2212.10 ms /    15 runs   (  147.47 ms per token,     6.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   33197.74 ms /   718 tokens\n",
      " 53%|█████████████████████████████████████████                                    | 81/152 [1:24:15<1:04:27, 54.47s/it]Llama.generate: 17 prefix-match hit, remaining 1298 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   58247.99 ms /  1298 tokens (   44.88 ms per token,    22.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2264.34 ms /    15 runs   (  150.96 ms per token,     6.62 tokens per second)\n",
      "llama_perf_context_print:       total time =   60523.07 ms /  1313 tokens\n",
      " 54%|█████████████████████████████████████████▌                                   | 82/152 [1:25:16<1:05:40, 56.29s/it]Llama.generate: 18 prefix-match hit, remaining 709 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   31067.67 ms /   709 tokens (   43.82 ms per token,    22.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2161.31 ms /    15 runs   (  144.09 ms per token,     6.94 tokens per second)\n",
      "llama_perf_context_print:       total time =   33238.48 ms /   724 tokens\n",
      " 55%|███████████████████████████████████████████▏                                   | 83/152 [1:25:49<56:47, 49.38s/it]Llama.generate: 18 prefix-match hit, remaining 1327 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   59505.17 ms /  1327 tokens (   44.84 ms per token,    22.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2275.61 ms /    15 runs   (  151.71 ms per token,     6.59 tokens per second)\n",
      "llama_perf_context_print:       total time =   61790.29 ms /  1342 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|██████████████████████████████████████████▌                                  | 84/152 [1:26:51<1:00:11, 53.11s/it]Llama.generate: 17 prefix-match hit, remaining 1108 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   49363.18 ms /  1108 tokens (   44.55 ms per token,    22.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2219.88 ms /    15 runs   (  147.99 ms per token,     6.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   51598.27 ms /  1123 tokens\n",
      " 56%|████████████████████████████████████████████▏                                  | 85/152 [1:27:43<58:48, 52.66s/it]Llama.generate: 17 prefix-match hit, remaining 761 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   33482.59 ms /   761 tokens (   44.00 ms per token,    22.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1494.88 ms /    10 runs   (  149.49 ms per token,     6.69 tokens per second)\n",
      "llama_perf_context_print:       total time =   34984.29 ms /   771 tokens\n",
      " 57%|████████████████████████████████████████████▋                                  | 86/152 [1:28:18<52:05, 47.36s/it]Llama.generate: 17 prefix-match hit, remaining 563 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   24926.30 ms /   563 tokens (   44.27 ms per token,    22.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2002.71 ms /    14 runs   (  143.05 ms per token,     6.99 tokens per second)\n",
      "llama_perf_context_print:       total time =   26937.97 ms /   577 tokens\n",
      " 57%|█████████████████████████████████████████████▏                                 | 87/152 [1:28:45<44:40, 41.24s/it]Llama.generate: 17 prefix-match hit, remaining 1042 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   46551.38 ms /  1042 tokens (   44.68 ms per token,    22.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2229.82 ms /    15 runs   (  148.65 ms per token,     6.73 tokens per second)\n",
      "llama_perf_context_print:       total time =   48795.48 ms /  1057 tokens\n",
      " 58%|█████████████████████████████████████████████▋                                 | 88/152 [1:29:33<46:24, 43.51s/it]Llama.generate: 17 prefix-match hit, remaining 1348 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   60319.44 ms /  1348 tokens (   44.75 ms per token,    22.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2257.03 ms /    15 runs   (  150.47 ms per token,     6.65 tokens per second)\n",
      "llama_perf_context_print:       total time =   62586.02 ms /  1363 tokens\n",
      " 59%|██████████████████████████████████████████████▎                                | 89/152 [1:30:36<51:42, 49.24s/it]Llama.generate: 17 prefix-match hit, remaining 915 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   40030.94 ms /   915 tokens (   43.75 ms per token,    22.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2204.78 ms /    15 runs   (  146.99 ms per token,     6.80 tokens per second)\n",
      "llama_perf_context_print:       total time =   42245.29 ms /   930 tokens\n",
      " 59%|██████████████████████████████████████████████▊                                | 90/152 [1:31:18<48:43, 47.15s/it]Llama.generate: 17 prefix-match hit, remaining 1803 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   82365.78 ms /  1803 tokens (   45.68 ms per token,    21.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2339.02 ms /    15 runs   (  155.93 ms per token,     6.41 tokens per second)\n",
      "llama_perf_context_print:       total time =   84714.68 ms /  1818 tokens\n",
      " 60%|███████████████████████████████████████████████▎                               | 91/152 [1:32:43<59:23, 58.42s/it]Llama.generate: 17 prefix-match hit, remaining 1193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   53211.29 ms /  1193 tokens (   44.60 ms per token,    22.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2236.89 ms /    15 runs   (  149.13 ms per token,     6.71 tokens per second)\n",
      "llama_perf_context_print:       total time =   55458.25 ms /  1208 tokens\n",
      " 61%|███████████████████████████████████████████████▊                               | 92/152 [1:33:38<57:32, 57.54s/it]Llama.generate: 17 prefix-match hit, remaining 408 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   17373.20 ms /   408 tokens (   42.58 ms per token,    23.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2122.69 ms /    15 runs   (  141.51 ms per token,     7.07 tokens per second)\n",
      "llama_perf_context_print:       total time =   19505.63 ms /   423 tokens\n",
      " 61%|████████████████████████████████████████████████▎                              | 93/152 [1:33:58<45:21, 46.13s/it]Llama.generate: 17 prefix-match hit, remaining 1166 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   52467.94 ms /  1166 tokens (   45.00 ms per token,    22.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2470.82 ms /    15 runs   (  164.72 ms per token,     6.07 tokens per second)\n",
      "llama_perf_context_print:       total time =   54948.56 ms /  1181 tokens\n",
      " 62%|████████████████████████████████████████████████▊                              | 94/152 [1:34:53<47:09, 48.78s/it]Llama.generate: 17 prefix-match hit, remaining 1985 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   91454.62 ms /  1985 tokens (   46.07 ms per token,    21.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2393.28 ms /    15 runs   (  159.55 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =   93857.73 ms /  2000 tokens\n",
      " 62%|█████████████████████████████████████████████████▍                             | 95/152 [1:36:27<59:11, 62.30s/it]Llama.generate: 17 prefix-match hit, remaining 1728 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   78558.68 ms /  1728 tokens (   45.46 ms per token,    22.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2319.08 ms /    15 runs   (  154.61 ms per token,     6.47 tokens per second)\n",
      "llama_perf_context_print:       total time =   80894.41 ms /  1743 tokens\n",
      " 63%|████████████████████████████████████████████████▋                            | 96/152 [1:37:48<1:03:21, 67.88s/it]Llama.generate: 17 prefix-match hit, remaining 1174 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   52406.51 ms /  1174 tokens (   44.64 ms per token,    22.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2399.04 ms /    15 runs   (  159.94 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =   54815.72 ms /  1189 tokens\n",
      " 64%|██████████████████████████████████████████████████▍                            | 97/152 [1:38:43<58:38, 63.97s/it]Llama.generate: 17 prefix-match hit, remaining 1455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   67509.13 ms /  1455 tokens (   46.40 ms per token,    21.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2378.01 ms /    15 runs   (  158.53 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =   69896.75 ms /  1470 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████████████████████████████████████████████████▉                            | 98/152 [1:39:52<59:10, 65.75s/it]Llama.generate: 17 prefix-match hit, remaining 968 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   42764.01 ms /   968 tokens (   44.18 ms per token,    22.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2210.40 ms /    15 runs   (  147.36 ms per token,     6.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   44983.97 ms /   983 tokens\n",
      " 65%|███████████████████████████████████████████████████▍                           | 99/152 [1:40:37<52:34, 59.52s/it]Llama.generate: 17 prefix-match hit, remaining 1357 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   61421.69 ms /  1357 tokens (   45.26 ms per token,    22.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2252.15 ms /    15 runs   (  150.14 ms per token,     6.66 tokens per second)\n",
      "llama_perf_context_print:       total time =   63683.21 ms /  1372 tokens\n",
      " 66%|███████████████████████████████████████████████████▎                          | 100/152 [1:41:41<52:40, 60.77s/it]Llama.generate: 18 prefix-match hit, remaining 869 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   38385.49 ms /   869 tokens (   44.17 ms per token,    22.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2221.49 ms /    15 runs   (  148.10 ms per token,     6.75 tokens per second)\n",
      "llama_perf_context_print:       total time =   40616.49 ms /   884 tokens\n",
      " 66%|███████████████████████████████████████████████████▊                          | 101/152 [1:42:22<46:31, 54.73s/it]Llama.generate: 17 prefix-match hit, remaining 1165 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   51397.38 ms /  1165 tokens (   44.12 ms per token,    22.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2239.50 ms /    15 runs   (  149.30 ms per token,     6.70 tokens per second)\n",
      "llama_perf_context_print:       total time =   53646.17 ms /  1180 tokens\n",
      " 67%|████████████████████████████████████████████████████▎                         | 102/152 [1:43:15<45:20, 54.41s/it]Llama.generate: 17 prefix-match hit, remaining 1557 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   71095.54 ms /  1557 tokens (   45.66 ms per token,    21.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2281.91 ms /    15 runs   (  152.13 ms per token,     6.57 tokens per second)\n",
      "llama_perf_context_print:       total time =   73387.21 ms /  1572 tokens\n",
      " 68%|████████████████████████████████████████████████████▊                         | 103/152 [1:44:29<49:05, 60.11s/it]Llama.generate: 17 prefix-match hit, remaining 1270 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   57050.48 ms /  1270 tokens (   44.92 ms per token,    22.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2474.34 ms /    15 runs   (  164.96 ms per token,     6.06 tokens per second)\n",
      "llama_perf_context_print:       total time =   59534.46 ms /  1285 tokens\n",
      " 68%|█████████████████████████████████████████████████████▎                        | 104/152 [1:45:28<47:57, 59.94s/it]Llama.generate: 17 prefix-match hit, remaining 1638 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   74354.45 ms /  1638 tokens (   45.39 ms per token,    22.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2325.86 ms /    15 runs   (  155.06 ms per token,     6.45 tokens per second)\n",
      "llama_perf_context_print:       total time =   76689.86 ms /  1653 tokens\n",
      " 69%|█████████████████████████████████████████████████████▉                        | 105/152 [1:46:45<50:53, 64.97s/it]Llama.generate: 17 prefix-match hit, remaining 1390 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   62406.40 ms /  1390 tokens (   44.90 ms per token,    22.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2275.96 ms /    15 runs   (  151.73 ms per token,     6.59 tokens per second)\n",
      "llama_perf_context_print:       total time =   64692.16 ms /  1405 tokens\n",
      " 70%|██████████████████████████████████████████████████████▍                       | 106/152 [1:47:50<49:45, 64.90s/it]Llama.generate: 17 prefix-match hit, remaining 751 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   32863.34 ms /   751 tokens (   43.76 ms per token,    22.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2173.54 ms /    15 runs   (  144.90 ms per token,     6.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   35046.38 ms /   766 tokens\n",
      " 70%|██████████████████████████████████████████████████████▉                       | 107/152 [1:48:25<41:57, 55.95s/it]Llama.generate: 17 prefix-match hit, remaining 1271 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   56903.55 ms /  1271 tokens (   44.77 ms per token,    22.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2456.02 ms /    15 runs   (  163.73 ms per token,     6.11 tokens per second)\n",
      "llama_perf_context_print:       total time =   59369.57 ms /  1286 tokens\n",
      " 71%|███████████████████████████████████████████████████████▍                      | 108/152 [1:49:24<41:47, 56.99s/it]Llama.generate: 17 prefix-match hit, remaining 1715 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   79168.29 ms /  1715 tokens (   46.16 ms per token,    21.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2314.93 ms /    15 runs   (  154.33 ms per token,     6.48 tokens per second)\n",
      "llama_perf_context_print:       total time =   81498.78 ms /  1730 tokens\n",
      " 72%|███████████████████████████████████████████████████████▉                      | 109/152 [1:50:46<46:06, 64.35s/it]Llama.generate: 17 prefix-match hit, remaining 1108 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   49816.13 ms /  1108 tokens (   44.96 ms per token,    22.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2244.30 ms /    15 runs   (  149.62 ms per token,     6.68 tokens per second)\n",
      "llama_perf_context_print:       total time =   52069.82 ms /  1123 tokens\n",
      " 72%|████████████████████████████████████████████████████████▍                     | 110/152 [1:51:38<42:28, 60.67s/it]Llama.generate: 17 prefix-match hit, remaining 1203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   53775.19 ms /  1203 tokens (   44.70 ms per token,    22.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2280.37 ms /    15 runs   (  152.02 ms per token,     6.58 tokens per second)\n",
      "llama_perf_context_print:       total time =   56065.20 ms /  1218 tokens\n",
      " 73%|████████████████████████████████████████████████████████▉                     | 111/152 [1:52:34<40:31, 59.29s/it]Llama.generate: 17 prefix-match hit, remaining 1246 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   55750.82 ms /  1246 tokens (   44.74 ms per token,    22.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2477.41 ms /    15 runs   (  165.16 ms per token,     6.05 tokens per second)\n",
      "llama_perf_context_print:       total time =   58237.89 ms /  1261 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|█████████████████████████████████████████████████████████▍                    | 112/152 [1:53:32<39:19, 58.98s/it]Llama.generate: 17 prefix-match hit, remaining 1606 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   73100.71 ms /  1606 tokens (   45.52 ms per token,    21.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2357.61 ms /    15 runs   (  157.17 ms per token,     6.36 tokens per second)\n",
      "llama_perf_context_print:       total time =   75468.06 ms /  1621 tokens\n",
      " 74%|█████████████████████████████████████████████████████████▉                    | 113/152 [1:54:48<41:33, 63.93s/it]Llama.generate: 17 prefix-match hit, remaining 959 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   42613.06 ms /   959 tokens (   44.43 ms per token,    22.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2215.45 ms /    15 runs   (  147.70 ms per token,     6.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   44837.77 ms /   974 tokens\n",
      " 75%|██████████████████████████████████████████████████████████▌                   | 114/152 [1:55:33<36:51, 58.21s/it]Llama.generate: 17 prefix-match hit, remaining 927 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   41022.16 ms /   927 tokens (   44.25 ms per token,    22.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2200.39 ms /    15 runs   (  146.69 ms per token,     6.82 tokens per second)\n",
      "llama_perf_context_print:       total time =   43231.88 ms /   942 tokens\n",
      " 76%|███████████████████████████████████████████████████████████                   | 115/152 [1:56:16<33:07, 53.72s/it]Llama.generate: 17 prefix-match hit, remaining 2271 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =  105037.57 ms /  2271 tokens (   46.25 ms per token,    21.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2597.08 ms /    15 runs   (  173.14 ms per token,     5.78 tokens per second)\n",
      "llama_perf_context_print:       total time =  107644.46 ms /  2286 tokens\n",
      " 76%|███████████████████████████████████████████████████████████▌                  | 116/152 [1:58:03<41:56, 69.90s/it]Llama.generate: 17 prefix-match hit, remaining 1297 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   58296.83 ms /  1297 tokens (   44.95 ms per token,    22.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2261.88 ms /    15 runs   (  150.79 ms per token,     6.63 tokens per second)\n",
      "llama_perf_context_print:       total time =   60568.55 ms /  1312 tokens\n",
      " 77%|████████████████████████████████████████████████████████████                  | 117/152 [1:59:04<39:08, 67.10s/it]Llama.generate: 17 prefix-match hit, remaining 1126 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   50344.25 ms /  1126 tokens (   44.71 ms per token,    22.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2237.71 ms /    15 runs   (  149.18 ms per token,     6.70 tokens per second)\n",
      "llama_perf_context_print:       total time =   52591.69 ms /  1141 tokens\n",
      " 78%|████████████████████████████████████████████████████████████▌                 | 118/152 [1:59:57<35:33, 62.75s/it]Llama.generate: 17 prefix-match hit, remaining 1148 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   50934.27 ms /  1148 tokens (   44.37 ms per token,    22.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2349.51 ms /    15 runs   (  156.63 ms per token,     6.38 tokens per second)\n",
      "llama_perf_context_print:       total time =   53293.40 ms /  1163 tokens\n",
      " 78%|█████████████████████████████████████████████████████████████                 | 119/152 [2:00:50<32:57, 59.92s/it]Llama.generate: 17 prefix-match hit, remaining 1593 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   72374.31 ms /  1593 tokens (   45.43 ms per token,    22.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2309.52 ms /    15 runs   (  153.97 ms per token,     6.49 tokens per second)\n",
      "llama_perf_context_print:       total time =   74693.65 ms /  1608 tokens\n",
      " 79%|█████████████████████████████████████████████████████████████▌                | 120/152 [2:02:05<34:19, 64.35s/it]Llama.generate: 17 prefix-match hit, remaining 2082 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   95763.78 ms /  2082 tokens (   46.00 ms per token,    21.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2364.54 ms /    15 runs   (  157.64 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =   98137.82 ms /  2097 tokens\n",
      " 80%|██████████████████████████████████████████████████████████████                | 121/152 [2:03:43<38:29, 74.49s/it]Llama.generate: 17 prefix-match hit, remaining 598 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   26249.29 ms /   598 tokens (   43.90 ms per token,    22.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2221.06 ms /    15 runs   (  148.07 ms per token,     6.75 tokens per second)\n",
      "llama_perf_context_print:       total time =   28480.71 ms /   613 tokens\n",
      " 80%|██████████████████████████████████████████████████████████████▌               | 122/152 [2:04:11<30:20, 60.69s/it]Llama.generate: 17 prefix-match hit, remaining 1265 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   56980.15 ms /  1265 tokens (   45.04 ms per token,    22.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2275.06 ms /    15 runs   (  151.67 ms per token,     6.59 tokens per second)\n",
      "llama_perf_context_print:       total time =   59264.71 ms /  1280 tokens\n",
      " 81%|███████████████████████████████████████████████████████████████               | 123/152 [2:05:11<29:07, 60.27s/it]Llama.generate: 17 prefix-match hit, remaining 1476 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   66833.67 ms /  1476 tokens (   45.28 ms per token,    22.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2280.46 ms /    15 runs   (  152.03 ms per token,     6.58 tokens per second)\n",
      "llama_perf_context_print:       total time =   69123.47 ms /  1491 tokens\n",
      " 82%|███████████████████████████████████████████████████████████████▋              | 124/152 [2:06:20<29:21, 62.93s/it]Llama.generate: 17 prefix-match hit, remaining 914 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   40637.15 ms /   914 tokens (   44.46 ms per token,    22.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2105.32 ms /    14 runs   (  150.38 ms per token,     6.65 tokens per second)\n",
      "llama_perf_context_print:       total time =   42751.33 ms /   928 tokens\n",
      " 82%|████████████████████████████████████████████████████████████████▏             | 125/152 [2:07:02<25:35, 56.87s/it]Llama.generate: 17 prefix-match hit, remaining 1439 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   64547.54 ms /  1439 tokens (   44.86 ms per token,    22.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2279.35 ms /    15 runs   (  151.96 ms per token,     6.58 tokens per second)\n",
      "llama_perf_context_print:       total time =   66836.36 ms /  1454 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████████████████████████████████████████████████████████████▋             | 126/152 [2:08:09<25:56, 59.87s/it]Llama.generate: 17 prefix-match hit, remaining 984 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   43570.07 ms /   984 tokens (   44.28 ms per token,    22.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2244.69 ms /    15 runs   (  149.65 ms per token,     6.68 tokens per second)\n",
      "llama_perf_context_print:       total time =   45824.02 ms /   999 tokens\n",
      " 84%|█████████████████████████████████████████████████████████████████▏            | 127/152 [2:08:55<23:11, 55.66s/it]Llama.generate: 17 prefix-match hit, remaining 1801 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   82220.35 ms /  1801 tokens (   45.65 ms per token,    21.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2315.11 ms /    15 runs   (  154.34 ms per token,     6.48 tokens per second)\n",
      "llama_perf_context_print:       total time =   84545.04 ms /  1816 tokens\n",
      " 84%|█████████████████████████████████████████████████████████████████▋            | 128/152 [2:10:20<25:43, 64.33s/it]Llama.generate: 17 prefix-match hit, remaining 1306 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   58693.54 ms /  1306 tokens (   44.94 ms per token,    22.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2245.88 ms /    15 runs   (  149.73 ms per token,     6.68 tokens per second)\n",
      "llama_perf_context_print:       total time =   60948.95 ms /  1321 tokens\n",
      " 85%|██████████████████████████████████████████████████████████████████▏           | 129/152 [2:11:21<24:16, 63.32s/it]Llama.generate: 17 prefix-match hit, remaining 954 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   42699.46 ms /   954 tokens (   44.76 ms per token,    22.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2424.82 ms /    15 runs   (  161.65 ms per token,     6.19 tokens per second)\n",
      "llama_perf_context_print:       total time =   45134.00 ms /   969 tokens\n",
      " 86%|██████████████████████████████████████████████████████████████████▋           | 130/152 [2:12:06<21:13, 57.87s/it]Llama.generate: 17 prefix-match hit, remaining 1001 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   43891.54 ms /  1001 tokens (   43.85 ms per token,    22.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2257.76 ms /    15 runs   (  150.52 ms per token,     6.64 tokens per second)\n",
      "llama_perf_context_print:       total time =   46159.06 ms /  1016 tokens\n",
      " 86%|███████████████████████████████████████████████████████████████████▏          | 131/152 [2:12:52<19:01, 54.36s/it]Llama.generate: 17 prefix-match hit, remaining 1024 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   45514.85 ms /  1024 tokens (   44.45 ms per token,    22.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2208.98 ms /    15 runs   (  147.27 ms per token,     6.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   47733.28 ms /  1039 tokens\n",
      " 87%|███████████████████████████████████████████████████████████████████▋          | 132/152 [2:13:40<17:27, 52.37s/it]Llama.generate: 17 prefix-match hit, remaining 1172 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   52239.90 ms /  1172 tokens (   44.57 ms per token,    22.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2242.76 ms /    15 runs   (  149.52 ms per token,     6.69 tokens per second)\n",
      "llama_perf_context_print:       total time =   54492.07 ms /  1187 tokens\n",
      " 88%|████████████████████████████████████████████████████████████████████▎         | 133/152 [2:14:34<16:47, 53.01s/it]Llama.generate: 17 prefix-match hit, remaining 1034 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   46024.83 ms /  1034 tokens (   44.51 ms per token,    22.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2406.79 ms /    15 runs   (  160.45 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =   48441.51 ms /  1049 tokens\n",
      " 88%|████████████████████████████████████████████████████████████████████▊         | 134/152 [2:15:23<15:29, 51.64s/it]Llama.generate: 17 prefix-match hit, remaining 1841 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   84254.29 ms /  1841 tokens (   45.77 ms per token,    21.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2320.22 ms /    15 runs   (  154.68 ms per token,     6.46 tokens per second)\n",
      "llama_perf_context_print:       total time =   86583.74 ms /  1856 tokens\n",
      " 89%|█████████████████████████████████████████████████████████████████████▎        | 135/152 [2:16:49<17:36, 62.14s/it]Llama.generate: 17 prefix-match hit, remaining 1293 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   57892.79 ms /  1293 tokens (   44.77 ms per token,    22.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2253.82 ms /    15 runs   (  150.25 ms per token,     6.66 tokens per second)\n",
      "llama_perf_context_print:       total time =   60156.21 ms /  1308 tokens\n",
      " 89%|█████████████████████████████████████████████████████████████████████▊        | 136/152 [2:17:49<16:24, 61.55s/it]Llama.generate: 17 prefix-match hit, remaining 1233 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   55373.25 ms /  1233 tokens (   44.91 ms per token,    22.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2254.87 ms /    15 runs   (  150.32 ms per token,     6.65 tokens per second)\n",
      "llama_perf_context_print:       total time =   57638.15 ms /  1248 tokens\n",
      " 90%|██████████████████████████████████████████████████████████████████████▎       | 137/152 [2:18:47<15:05, 60.37s/it]Llama.generate: 17 prefix-match hit, remaining 1049 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   46985.76 ms /  1049 tokens (   44.79 ms per token,    22.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2221.10 ms /    15 runs   (  148.07 ms per token,     6.75 tokens per second)\n",
      "llama_perf_context_print:       total time =   49216.35 ms /  1064 tokens\n",
      " 91%|██████████████████████████████████████████████████████████████████████▊       | 138/152 [2:19:36<13:18, 57.03s/it]Llama.generate: 17 prefix-match hit, remaining 650 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   28279.72 ms /   650 tokens (   43.51 ms per token,    22.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2156.62 ms /    15 runs   (  143.77 ms per token,     6.96 tokens per second)\n",
      "llama_perf_context_print:       total time =   30446.11 ms /   665 tokens\n",
      " 91%|███████████████████████████████████████████████████████████████████████▎      | 139/152 [2:20:07<10:37, 49.06s/it]Llama.generate: 17 prefix-match hit, remaining 1847 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   84993.43 ms /  1847 tokens (   46.02 ms per token,    21.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2440.27 ms /    15 runs   (  162.68 ms per token,     6.15 tokens per second)\n",
      "llama_perf_context_print:       total time =   87448.49 ms /  1862 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████████████████████████████████████████▊      | 140/152 [2:21:34<12:06, 60.58s/it]Llama.generate: 17 prefix-match hit, remaining 1780 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   81315.86 ms /  1780 tokens (   45.68 ms per token,    21.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2335.56 ms /    15 runs   (  155.70 ms per token,     6.42 tokens per second)\n",
      "llama_perf_context_print:       total time =   83677.25 ms /  1795 tokens\n",
      " 93%|████████████████████████████████████████████████████████████████████████▎     | 141/152 [2:22:58<12:22, 67.51s/it]Llama.generate: 17 prefix-match hit, remaining 1320 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   59237.30 ms /  1320 tokens (   44.88 ms per token,    22.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2257.43 ms /    15 runs   (  150.50 ms per token,     6.64 tokens per second)\n",
      "llama_perf_context_print:       total time =   61504.40 ms /  1335 tokens\n",
      " 93%|████████████████████████████████████████████████████████████████████████▊     | 142/152 [2:23:59<10:57, 65.71s/it]Llama.generate: 17 prefix-match hit, remaining 898 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   39816.25 ms /   898 tokens (   44.34 ms per token,    22.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2225.83 ms /    15 runs   (  148.39 ms per token,     6.74 tokens per second)\n",
      "llama_perf_context_print:       total time =   42051.44 ms /   913 tokens\n",
      " 94%|█████████████████████████████████████████████████████████████████████████▍    | 143/152 [2:24:41<08:47, 58.62s/it]Llama.generate: 17 prefix-match hit, remaining 1484 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   67242.87 ms /  1484 tokens (   45.31 ms per token,    22.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2287.87 ms /    15 runs   (  152.52 ms per token,     6.56 tokens per second)\n",
      "llama_perf_context_print:       total time =   69540.21 ms /  1499 tokens\n",
      " 95%|█████████████████████████████████████████████████████████████████████████▉    | 144/152 [2:25:51<08:15, 61.89s/it]Llama.generate: 17 prefix-match hit, remaining 1034 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   45888.01 ms /  1034 tokens (   44.38 ms per token,    22.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2209.77 ms /    15 runs   (  147.32 ms per token,     6.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   48107.19 ms /  1049 tokens\n",
      " 95%|██████████████████████████████████████████████████████████████████████████▍   | 145/152 [2:26:39<06:44, 57.76s/it]Llama.generate: 17 prefix-match hit, remaining 1621 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   73465.01 ms /  1621 tokens (   45.32 ms per token,    22.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2315.16 ms /    15 runs   (  154.34 ms per token,     6.48 tokens per second)\n",
      "llama_perf_context_print:       total time =   75789.57 ms /  1636 tokens\n",
      " 96%|██████████████████████████████████████████████████████████████████████████▉   | 146/152 [2:27:55<06:19, 63.18s/it]Llama.generate: 17 prefix-match hit, remaining 1307 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   59142.10 ms /  1307 tokens (   45.25 ms per token,    22.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2257.08 ms /    15 runs   (  150.47 ms per token,     6.65 tokens per second)\n",
      "llama_perf_context_print:       total time =   61408.81 ms /  1322 tokens\n",
      " 97%|███████████████████████████████████████████████████████████████████████████▍  | 147/152 [2:28:56<05:13, 62.65s/it]Llama.generate: 17 prefix-match hit, remaining 1666 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   75690.84 ms /  1666 tokens (   45.43 ms per token,    22.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2467.65 ms /    15 runs   (  164.51 ms per token,     6.08 tokens per second)\n",
      "llama_perf_context_print:       total time =   78169.45 ms /  1681 tokens\n",
      " 97%|███████████████████████████████████████████████████████████████████████████▉  | 148/152 [2:30:15<04:29, 67.31s/it]Llama.generate: 17 prefix-match hit, remaining 1288 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   57589.89 ms /  1288 tokens (   44.71 ms per token,    22.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2276.59 ms /    15 runs   (  151.77 ms per token,     6.59 tokens per second)\n",
      "llama_perf_context_print:       total time =   59875.74 ms /  1303 tokens\n",
      " 98%|████████████████████████████████████████████████████████████████████████████▍ | 149/152 [2:31:14<03:15, 65.08s/it]Llama.generate: 17 prefix-match hit, remaining 918 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   40575.99 ms /   918 tokens (   44.20 ms per token,    22.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2196.95 ms /    15 runs   (  146.46 ms per token,     6.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   42782.41 ms /   933 tokens\n",
      " 99%|████████████████████████████████████████████████████████████████████████████▉ | 150/152 [2:31:57<01:56, 58.39s/it]Llama.generate: 18 prefix-match hit, remaining 1241 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   55852.64 ms /  1241 tokens (   45.01 ms per token,    22.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1955.75 ms /    13 runs   (  150.44 ms per token,     6.65 tokens per second)\n",
      "llama_perf_context_print:       total time =   57816.70 ms /  1254 tokens\n",
      " 99%|█████████████████████████████████████████████████████████████████████████████▍| 151/152 [2:32:55<00:58, 58.22s/it]Llama.generate: 17 prefix-match hit, remaining 1444 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   64703.10 ms /  1444 tokens (   44.81 ms per token,    22.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2272.12 ms /    15 runs   (  151.47 ms per token,     6.60 tokens per second)\n",
      "llama_perf_context_print:       total time =   66984.72 ms /  1459 tokens\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 152/152 [2:34:02<00:00, 60.81s/it]\n",
      "2025-03-23 02:39:08,612 - BERTopic - Representation - Completed ✓\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load embedding model (optimized for speed & performance)\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cuda\")\n",
    "\n",
    "# ✅ Optimized UMAP: Reduce dimensionality while retaining local structure\n",
    "umap_model = umap.UMAP(\n",
    "    n_neighbors=10,  # Reduced to retain local document similarities\n",
    "    n_components=10,  # Increased from 5 to preserve more information\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Optimized HDBSCAN: Reduce outlier sensitivity\n",
    "hdbscan_model = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=500,  # Reduced from 1000 (allows smaller clusters)\n",
    "    min_samples=5,  # Reduced from 10 (reduces noise sensitivity)\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='leaf',  # More fine-grained clustering\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "# ✅ Optimized CountVectorizer: Retain rare but meaningful words\n",
    "vectorizer_model = CountVectorizer(\n",
    "    max_df=0.98,  # Increased to retain more common words\n",
    "    min_df=3,  # Reduced to allow rarer words\n",
    "    stop_words='english',  # Stopwords removal\n",
    "    ngram_range=(1, 2)  # Allow bi-grams for better topic coherence\n",
    ")\n",
    "\n",
    "#Explicit classbased TF-IDF representation\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "# Initialize BERTopic with optimized settings\n",
    "bertopic_model = BERTopic(\n",
    "    representation_model=representation_model,\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Fit BERTopic model\n",
    "topics, probs = bertopic_model.fit_transform(df[\"processed_text\"].tolist())\n",
    "\n",
    "# Extract topics, filtering out -1 (outliers)\n",
    "topic_words_list = [\n",
    "    [word for word, _ in bertopic_model.get_topic(topic_id)]\n",
    "    for topic_id in bertopic_model.get_topics().keys() if topic_id != -1\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efd58849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 02:39:17,115 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    }
   ],
   "source": [
    "# Define save path\n",
    "save_path = \"C:/Users/User/OneDrive - Singapore Management University/Semester 5/ISSS609 Text Analytics and Applications/Group Project/Amazon Fine Food Reviews/Clean/Experiment_3 - base bertopic + cTFIDF + LLM Rep/bertopic_model_v5\"\n",
    "\n",
    "# # Save the trained BERTopic model\n",
    "bertopic_model.save(save_path)\n",
    "# print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7afddd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "# Load the saved model\n",
    "bertopic_model = BERTopic.load(save_path)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe8be951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Get the c-TF-IDF matrix and vocabulary\n",
    "ctfidf = bertopic_model.c_tf_idf_\n",
    "vocab = bertopic_model.vectorizer_model.get_feature_names_out()\n",
    "\n",
    "# Get topic-term weights for each topic\n",
    "topic_words_list = []\n",
    "for row in ctfidf:\n",
    "    # Get indices of top 10 terms per topic\n",
    "    top_indices = row.toarray().flatten().argsort()[::-1][:10]\n",
    "    top_words = [vocab[i] for i in top_indices]\n",
    "    topic_words_list.append(top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b75943ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: ['bar', 'cat', 'blend', 'chocolate', 'candy', 'roast', 'keurig', 'dark', 'green', 'chicken']\n",
      "Topic 1: ['dog food', 'dog', 'food dog', 'beneful', 'puppy', 'kibble', 'stool', 'coat', 'dry dog', 'feeding']\n",
      "Topic 2: ['email', 'business', 'timely', 'wa exactly', 'product arrived', 'seller', 'transaction', 'vendor', 'item wa', 'great service']\n",
      "Topic 3: ['popcorn', 'kernel', 'popper', 'theater', 'movie', 'popped', 'pop', 'popping', 'movie theater', 'microwave popcorn']\n",
      "Topic 4: ['love tea', 'tea bag', 'tea', 'tea wa', 'flowering', 'flowering tea', 'teapot', 'loose', 'loose tea', 'box tea']\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(topic_words_list[:5]):\n",
    "    print(f\"Topic {i}: {topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c7e8088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Coherence (c_v): 0.7720\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Flatten multi-word phrases into individual tokens\n",
    "flattened_topic_words_list = []\n",
    "for topic in topic_words_list:\n",
    "    clean_topic = []\n",
    "    for word in topic:\n",
    "        clean_topic.extend(word.split())  # split multi-word expressions like \"pop chip\"\n",
    "    flattened_topic_words_list.append(clean_topic)\n",
    "\n",
    "# Tokenize documents\n",
    "tokenized_docs = df[\"processed_text\"].apply(lambda x: x.split()).tolist()\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.8)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "\n",
    "# Compute coherence\n",
    "coherence_model = CoherenceModel(\n",
    "    topics=flattened_topic_words_list,\n",
    "    texts=tokenized_docs,\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v'\n",
    ")\n",
    "\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Topic Coherence (c_v): {coherence_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e743630a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score (c_v): 0.772043193776765\n",
      "Diversity Score: 0.9210526315789473\n",
      "Number of Topics: 152\n"
     ]
    }
   ],
   "source": [
    "# Compute Topic Diversity\n",
    "all_words = [word for topic in topic_words_list for word in topic]\n",
    "unique_words = set(all_words)\n",
    "diversity_score = len(unique_words) / len(all_words) if len(all_words) > 0 else 0\n",
    "\n",
    "# Print results\n",
    "print(f\"Coherence Score (c_v): {coherence_score}\")\n",
    "print(f\"Diversity Score: {diversity_score}\")\n",
    "print(f\"Number of Topics: {len(topic_words_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f959b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 3 prefix-match hit, remaining 216 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =   23937.06 ms /   216 tokens (  110.82 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =   36379.66 ms /   195 runs   (  186.56 ms per token,     5.36 tokens per second)\n",
      "llama_perf_context_print:       total time =   60961.99 ms /   411 tokens\n",
      "Llama.generate: 35 prefix-match hit, remaining 176 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    7517.76 ms /   176 tokens (   42.71 ms per token,    23.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =   43937.60 ms /   299 runs   (  146.95 ms per token,     6.81 tokens per second)\n",
      "llama_perf_context_print:       total time =   51814.68 ms /   475 tokens\n",
      "Llama.generate: 35 prefix-match hit, remaining 184 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    7934.43 ms /   184 tokens (   43.12 ms per token,    23.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =   41812.11 ms /   294 runs   (  142.22 ms per token,     7.03 tokens per second)\n",
      "llama_perf_context_print:       total time =   50002.53 ms /   478 tokens\n",
      "Llama.generate: 35 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8031.51 ms /   187 tokens (   42.95 ms per token,    23.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =   30276.08 ms /   215 runs   (  140.82 ms per token,     7.10 tokens per second)\n",
      "llama_perf_context_print:       total time =   38396.06 ms /   402 tokens\n",
      "Llama.generate: 35 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8473.28 ms /   194 tokens (   43.68 ms per token,    22.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24365.99 ms /   171 runs   (  142.49 ms per token,     7.02 tokens per second)\n",
      "llama_perf_context_print:       total time =   32895.19 ms /   365 tokens\n",
      "Llama.generate: 35 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8837.98 ms /   199 tokens (   44.41 ms per token,    22.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =   19500.07 ms /   137 runs   (  142.34 ms per token,     7.03 tokens per second)\n",
      "llama_perf_context_print:       total time =   28426.46 ms /   336 tokens\n",
      "Llama.generate: 35 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8726.54 ms /   200 tokens (   43.63 ms per token,    22.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =   34667.01 ms /   243 runs   (  142.66 ms per token,     7.01 tokens per second)\n",
      "llama_perf_context_print:       total time =   43514.37 ms /   443 tokens\n",
      "Llama.generate: 35 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8210.20 ms /   193 tokens (   42.54 ms per token,    23.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =   16416.29 ms /   114 runs   (  144.00 ms per token,     6.94 tokens per second)\n",
      "llama_perf_context_print:       total time =   24658.49 ms /   307 tokens\n",
      "Llama.generate: 35 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    7957.86 ms /   189 tokens (   42.11 ms per token,    23.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18628.74 ms /   130 runs   (  143.30 ms per token,     6.98 tokens per second)\n",
      "llama_perf_context_print:       total time =   26625.19 ms /   319 tokens\n",
      "Llama.generate: 35 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    7808.04 ms /   185 tokens (   42.21 ms per token,    23.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =   31623.37 ms /   221 runs   (  143.09 ms per token,     6.99 tokens per second)\n",
      "llama_perf_context_print:       total time =   39510.51 ms /   406 tokens\n"
     ]
    }
   ],
   "source": [
    "# Get topic frequency from BERTopic\n",
    "topic_freq = bertopic_model.get_topic_freq()\n",
    "\n",
    "# Remove outlier topic (-1)\n",
    "topic_freq = topic_freq[topic_freq.Topic != -1]\n",
    "\n",
    "# Get top 10 topic IDs by frequency\n",
    "top_10_topic_ids = topic_freq.head(10)[\"Topic\"].tolist()\n",
    "\n",
    "topic_insights = []\n",
    "for topic_id in top_10_topic_ids:\n",
    "    keywords = topic_words_list[topic_id]\n",
    "    keywords_str = \", \".join(keywords)\n",
    "    \n",
    "    reps = bertopic_model.get_representative_docs(topic_id)\n",
    "    example_doc = reps[0][:300] if reps else \"N/A\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an NLP expert working for Amazon under the Fine Food Departments. Here's the output of a topic modeling analysis.\n",
    "\n",
    "Topic {topic_id} Keywords: {keywords_str}\n",
    "\n",
    "Representative Document: \"{example_doc}\"\n",
    "\n",
    "Topic Coherence Score: {coherence_score:.4f}\n",
    "Topic Diversity Score: {diversity_score:.4f}\n",
    "\n",
    "Please:\n",
    "1. Briefly explain what this topic is about.\n",
    "2. Suggest how this topic might be useful for Amazon and how the Fine Food Departments, in terms of product characteristic or demands.\n",
    "\n",
    "Be concise and insightful.\n",
    "\"\"\"\n",
    "\n",
    "    result = llm(prompt, max_tokens=300)\n",
    "    topic_insights.append({\n",
    "        \"Topic\": topic_id,\n",
    "        \"Keywords\": keywords_str,\n",
    "        \"Insight\": result[\"choices\"][0][\"text\"]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eaf189a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "insight_df = pd.DataFrame(topic_insights)\n",
    "insight_df.to_csv(\"C:/Users/User/OneDrive - Singapore Management University/Semester 5/ISSS609 Text Analytics and Applications/Group Project/Amazon Fine Food Reviews/Clean/Experiment_3 - base bertopic + cTFIDF + LLM Rep/top_10_llm_topic_insights_v1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52ca6e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 35 prefix-match hit, remaining 184 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    6534.07 ms /   184 tokens (   35.51 ms per token,    28.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22527.08 ms /   160 runs   (  140.79 ms per token,     7.10 tokens per second)\n",
      "llama_perf_context_print:       total time =   29129.13 ms /   344 tokens\n",
      "Llama.generate: 35 prefix-match hit, remaining 176 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    6697.25 ms /   176 tokens (   38.05 ms per token,    26.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =   35737.26 ms /   254 runs   (  140.70 ms per token,     7.11 tokens per second)\n",
      "llama_perf_context_print:       total time =   42814.79 ms /   430 tokens\n",
      "Llama.generate: 35 prefix-match hit, remaining 184 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    7777.00 ms /   184 tokens (   42.27 ms per token,    23.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21740.85 ms /   156 runs   (  139.36 ms per token,     7.18 tokens per second)\n",
      "llama_perf_context_print:       total time =   29676.02 ms /   340 tokens\n",
      "Llama.generate: 35 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    7830.66 ms /   187 tokens (   41.88 ms per token,    23.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =   35936.89 ms /   257 runs   (  139.83 ms per token,     7.15 tokens per second)\n",
      "llama_perf_context_print:       total time =   43906.40 ms /   444 tokens\n",
      "Llama.generate: 35 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8306.25 ms /   194 tokens (   42.82 ms per token,    23.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =   29980.06 ms /   211 runs   (  142.09 ms per token,     7.04 tokens per second)\n",
      "llama_perf_context_print:       total time =   38360.38 ms /   405 tokens\n",
      "Llama.generate: 35 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    9305.04 ms /   199 tokens (   46.76 ms per token,    21.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22850.37 ms /   151 runs   (  151.33 ms per token,     6.61 tokens per second)\n",
      "llama_perf_context_print:       total time =   32221.47 ms /   350 tokens\n",
      "Llama.generate: 35 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    9508.76 ms /   200 tokens (   47.54 ms per token,    21.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =   43709.54 ms /   299 runs   (  146.19 ms per token,     6.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   53376.51 ms /   499 tokens\n",
      "Llama.generate: 35 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8457.06 ms /   193 tokens (   43.82 ms per token,    22.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =   27487.23 ms /   195 runs   (  140.96 ms per token,     7.09 tokens per second)\n",
      "llama_perf_context_print:       total time =   36074.44 ms /   388 tokens\n",
      "Llama.generate: 35 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    7862.00 ms /   189 tokens (   41.60 ms per token,    24.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21242.32 ms /   150 runs   (  141.62 ms per token,     7.06 tokens per second)\n",
      "llama_perf_context_print:       total time =   29214.44 ms /   339 tokens\n",
      "Llama.generate: 35 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    7788.99 ms /   185 tokens (   42.10 ms per token,    23.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =   40146.91 ms /   283 runs   (  141.86 ms per token,     7.05 tokens per second)\n",
      "llama_perf_context_print:       total time =   48049.24 ms /   468 tokens\n",
      "Llama.generate: 35 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8371.25 ms /   198 tokens (   42.28 ms per token,    23.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22709.31 ms /   163 runs   (  139.32 ms per token,     7.18 tokens per second)\n",
      "llama_perf_context_print:       total time =   31242.77 ms /   361 tokens\n",
      "Llama.generate: 36 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8539.65 ms /   191 tokens (   44.71 ms per token,    22.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =   29025.20 ms /   208 runs   (  139.54 ms per token,     7.17 tokens per second)\n",
      "llama_perf_context_print:       total time =   37635.64 ms /   399 tokens\n",
      "Llama.generate: 36 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8799.93 ms /   202 tokens (   43.56 ms per token,    22.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =   25774.61 ms /   183 runs   (  140.84 ms per token,     7.10 tokens per second)\n",
      "llama_perf_context_print:       total time =   34662.53 ms /   385 tokens\n",
      "Llama.generate: 36 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8134.55 ms /   189 tokens (   43.04 ms per token,    23.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =   25876.79 ms /   184 runs   (  140.63 ms per token,     7.11 tokens per second)\n",
      "llama_perf_context_print:       total time =   34075.94 ms /   373 tokens\n",
      "Llama.generate: 36 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8510.48 ms /   196 tokens (   43.42 ms per token,    23.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =   30563.59 ms /   217 runs   (  140.85 ms per token,     7.10 tokens per second)\n",
      "llama_perf_context_print:       total time =   39150.10 ms /   413 tokens\n",
      "Llama.generate: 36 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8191.42 ms /   192 tokens (   42.66 ms per token,    23.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21718.12 ms /   151 runs   (  143.83 ms per token,     6.95 tokens per second)\n",
      "llama_perf_context_print:       total time =   29961.06 ms /   343 tokens\n",
      "Llama.generate: 36 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8392.67 ms /   198 tokens (   42.39 ms per token,    23.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =   19934.07 ms /   139 runs   (  143.41 ms per token,     6.97 tokens per second)\n",
      "llama_perf_context_print:       total time =   28390.49 ms /   337 tokens\n",
      "Llama.generate: 36 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    7900.39 ms /   188 tokens (   42.02 ms per token,    23.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28219.63 ms /   198 runs   (  142.52 ms per token,     7.02 tokens per second)\n",
      "llama_perf_context_print:       total time =   36261.53 ms /   386 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 36 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8654.29 ms /   199 tokens (   43.49 ms per token,    22.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =   42322.36 ms /   299 runs   (  141.55 ms per token,     7.06 tokens per second)\n",
      "llama_perf_context_print:       total time =   51905.59 ms /   498 tokens\n",
      "Llama.generate: 36 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8233.75 ms /   191 tokens (   43.11 ms per token,    23.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =   42411.31 ms /   299 runs   (  141.84 ms per token,     7.05 tokens per second)\n",
      "llama_perf_context_print:       total time =   50775.16 ms /   490 tokens\n",
      "Llama.generate: 35 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8340.97 ms /   189 tokens (   44.13 ms per token,    22.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =   31401.08 ms /   221 runs   (  142.09 ms per token,     7.04 tokens per second)\n",
      "llama_perf_context_print:       total time =   39821.71 ms /   410 tokens\n",
      "Llama.generate: 36 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8280.06 ms /   196 tokens (   42.25 ms per token,    23.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24863.91 ms /   175 runs   (  142.08 ms per token,     7.04 tokens per second)\n",
      "llama_perf_context_print:       total time =   33200.94 ms /   371 tokens\n",
      "Llama.generate: 36 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8290.51 ms /   194 tokens (   42.73 ms per token,    23.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   41059.42 ms /   289 runs   (  142.07 ms per token,     7.04 tokens per second)\n",
      "llama_perf_context_print:       total time =   49477.29 ms /   483 tokens\n",
      "Llama.generate: 36 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8155.78 ms /   193 tokens (   42.26 ms per token,    23.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =   33908.34 ms /   238 runs   (  142.47 ms per token,     7.02 tokens per second)\n",
      "llama_perf_context_print:       total time =   42206.41 ms /   431 tokens\n",
      "Llama.generate: 36 prefix-match hit, remaining 183 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8018.23 ms /   183 tokens (   43.82 ms per token,    22.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28130.49 ms /   199 runs   (  141.36 ms per token,     7.07 tokens per second)\n",
      "llama_perf_context_print:       total time =   36232.74 ms /   382 tokens\n",
      "Llama.generate: 36 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8429.97 ms /   189 tokens (   44.60 ms per token,    22.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =   25345.07 ms /   180 runs   (  140.81 ms per token,     7.10 tokens per second)\n",
      "llama_perf_context_print:       total time =   33833.65 ms /   369 tokens\n",
      "Llama.generate: 36 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8122.92 ms /   192 tokens (   42.31 ms per token,    23.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22934.99 ms /   163 runs   (  140.71 ms per token,     7.11 tokens per second)\n",
      "llama_perf_context_print:       total time =   31108.88 ms /   355 tokens\n",
      "Llama.generate: 36 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8729.17 ms /   206 tokens (   42.37 ms per token,    23.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24921.25 ms /   178 runs   (  140.01 ms per token,     7.14 tokens per second)\n",
      "llama_perf_context_print:       total time =   33707.00 ms /   384 tokens\n",
      "Llama.generate: 36 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    9014.35 ms /   203 tokens (   44.41 ms per token,    22.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =   23292.74 ms /   166 runs   (  140.32 ms per token,     7.13 tokens per second)\n",
      "llama_perf_context_print:       total time =   32358.96 ms /   369 tokens\n",
      "Llama.generate: 36 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   75643.54 ms\n",
      "llama_perf_context_print: prompt eval time =    8273.28 ms /   190 tokens (   43.54 ms per token,    22.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24012.01 ms /   170 runs   (  141.25 ms per token,     7.08 tokens per second)\n",
      "llama_perf_context_print:       total time =   32339.62 ms /   360 tokens\n"
     ]
    }
   ],
   "source": [
    "# Get topic frequency from BERTopic\n",
    "topic_freq = bertopic_model.get_topic_freq()\n",
    "\n",
    "# Remove outlier topic (-1)\n",
    "topic_freq = topic_freq[topic_freq.Topic != -1]\n",
    "\n",
    "# Get top 30 topic IDs by frequency\n",
    "top_30_topic_ids = topic_freq.head(30)[\"Topic\"].tolist()\n",
    "\n",
    "topic_insights = []\n",
    "for topic_id in top_30_topic_ids:\n",
    "    keywords = topic_words_list[topic_id]\n",
    "    keywords_str = \", \".join(keywords)\n",
    "    \n",
    "    reps = bertopic_model.get_representative_docs(topic_id)\n",
    "    example_doc = reps[0][:300] if reps else \"N/A\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an NLP expert working for Amazon under the Fine Food Departments. Here's the output of a topic modeling analysis.\n",
    "\n",
    "Topic {topic_id} Keywords: {keywords_str}\n",
    "\n",
    "Representative Document: \"{example_doc}\"\n",
    "\n",
    "Topic Coherence Score: {coherence_score:.4f}\n",
    "Topic Diversity Score: {diversity_score:.4f}\n",
    "\n",
    "Please:\n",
    "1. Briefly explain what this topic is about.\n",
    "2. Suggest how this topic might be useful for Amazon and how the Fine Food Departments, in terms of product characteristic or demands.\n",
    "\n",
    "Be concise and insightful.\n",
    "\"\"\"\n",
    "\n",
    "    result = llm(prompt, max_tokens=300)\n",
    "    topic_insights.append({\n",
    "        \"Topic\": topic_id,\n",
    "        \"Keywords\": keywords_str,\n",
    "        \"Insight\": result[\"choices\"][0][\"text\"]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f7a87d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "insight_df = pd.DataFrame(topic_insights)\n",
    "insight_df.to_csv(\"C:/Users/User/OneDrive - Singapore Management University/Semester 5/ISSS609 Text Analytics and Applications/Group Project/Amazon Fine Food Reviews/Clean/Experiment_3 - base bertopic + cTFIDF + LLM Rep/top_30_llm_topic_insights_v1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ffccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top topic frequencies, excluding -1\n",
    "topic_freq = bertopic_model.get_topic_freq()\n",
    "topic_freq = topic_freq[topic_freq[\"Topic\"] != -1].head(20)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(topic_freq[\"Topic\"].astype(str), topic_freq[\"Count\"], color=\"skyblue\")\n",
    "plt.xlabel(\"Topic Number\")\n",
    "plt.ylabel(\"Number of Documents\")\n",
    "plt.title(\"Top 20 Topics by Frequency (Excluding Outliers)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39dab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top topic frequencies, including -1, outliers\n",
    "topic_freq = bertopic_model.get_topic_freq().head(20)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(topic_freq[\"Topic\"].astype(str), topic_freq[\"Count\"], color=\"skyblue\")\n",
    "plt.xlabel(\"Topic Number\")\n",
    "plt.ylabel(\"Number of Documents\")\n",
    "plt.title(\"Top 10 Topics by Frequency\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1027aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Assigned_Topic' column exists\n",
    "if \"Assigned_Topic\" not in df.columns:\n",
    "    df[\"Assigned_Topic\"] = topics  # Assign topics\n",
    "\n",
    "# Extract sample reviews for topics -1 to 19\n",
    "for topic_num in range(-1, 20):\n",
    "    print(f\"\\n🔹 Sample Reviews for Topic {topic_num}:\")\n",
    "\n",
    "    # Filter the DataFrame for the given topic\n",
    "    topic_reviews = df[df[\"Assigned_Topic\"] == topic_num][\"Text\"]\n",
    "\n",
    "    # Check if reviews exist for the topic\n",
    "    if not topic_reviews.empty:\n",
    "        sample_reviews = topic_reviews.sample(min(3, len(topic_reviews)), random_state=42).tolist()\n",
    "    else:\n",
    "        sample_reviews = [\"No reviews available for this topic.\"]\n",
    "\n",
    "    print(sample_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98d3368",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df[\"processed_text\"].apply(lambda x: \" \".join(x)).tolist()\n",
    "timestamps = df[\"DateTime\"].tolist()\n",
    "topics, _ = bertopic_model.transform(docs)\n",
    "\n",
    "# Add topic & timestamp back into a DataFrame\n",
    "df_topics = pd.DataFrame({\n",
    "    \"Document\": docs,\n",
    "    \"Timestamp\": timestamps,\n",
    "    \"Topic\": topics\n",
    "})\n",
    "\n",
    "# Get top 10 topics\n",
    "top_10_topic_ids = (\n",
    "    bertopic_model.get_topic_freq()\n",
    "    .query(\"Topic != -1\")\n",
    "    .head(10)[\"Topic\"]\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# Filter to only those documents\n",
    "filtered_df = df_topics[df_topics.Topic.isin(top_10_topic_ids)]\n",
    "\n",
    "# Then recompute topics_over_time on only those docs\n",
    "topics_over_time_filtered = bertopic_model.topics_over_time(\n",
    "    docs=filtered_df[\"Document\"].tolist(),\n",
    "    timestamps=filtered_df[\"Timestamp\"].tolist(),\n",
    "    topics=filtered_df[\"Topic\"].tolist(),\n",
    "    nr_bins=15\n",
    ")\n",
    "\n",
    "# Plot\n",
    "fig = bertopic_model.visualize_topics_over_time(\n",
    "    topics_over_time_filtered,\n",
    "    topics=top_10_topic_ids\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16b338b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
